{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ptt Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lib\n",
    "import numpy as np\n",
    "import requests\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# date, title, url\n",
    "articles = []\n",
    "popular_articles = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crawl data in 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def crawl(url, all_articles_2017, all_articles_popular_2017):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    if counter is 10:\n",
    "        return\n",
    "    \n",
    "    find_2016 = False\n",
    "    ptt_url = \"https://www.ptt.cc\"\n",
    "    r = requests.get(url)\n",
    "    content = r.text\n",
    "    page_soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    # Find articles\n",
    "    articles_class = page_soup.find_all(class_=\"r-ent\")\n",
    "    for article in articles_class:\n",
    "        \n",
    "        if article.a is not None:\n",
    "            if article.a.text.find(\"公告\") == -1:\n",
    "                \n",
    "                # Find push counter\n",
    "                push_counter_string = article.find(\"div\", \"nrec\").string\n",
    "                if push_counter_string is None:\n",
    "                    push_counter_string = \"0\"\n",
    "                \n",
    "                # Find title and url\n",
    "                title = article.a.string\n",
    "                article_url = article.a.get('href')\n",
    "                article_url = ptt_url + article_url\n",
    "                \n",
    "                # the year and date of article\n",
    "                year = get_article_year(article_url)\n",
    "                date = article.find(\"div\", \"date\").string\n",
    "                date = date.split(\"/\")[0]+date.split(\"/\")[1]\n",
    "                 \n",
    "                time.sleep(0.01)\n",
    "                \n",
    "                # Get article info (date, push_counter, title url)\n",
    "                if year.find(\"2017\") !=-1:\n",
    "                    article_info = date + \" \" + title + \" \" + article_url\n",
    "                    print (date)\n",
    "                    all_articles_2017.append(article_info)\n",
    "                    if push_counter_string.find(\"爆\") is not -1:\n",
    "                        all_articles_popular_2017.append(article_info)\n",
    "                elif year.find(\"2016\") != -1:\n",
    "                    find_2016 = True\n",
    "    \n",
    "    # Find previous page url \n",
    "    time.sleep(0.01)\n",
    "    if find_2016:\n",
    "        return\n",
    "    \n",
    "    page_button_class = page_soup.find_all(class_=\"btn wide\")\n",
    "    for page_button in page_button_class:\n",
    "        if page_button.text is not None:\n",
    "            if page_button.text.find(\"上頁\") != -1:\n",
    "                previos_page_url = page_button.get('href')\n",
    "                previos_page_url = ptt_url + previos_page_url\n",
    "                crawl(previos_page_url, all_articles_2017, all_articles_popular_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_article_year(url):\n",
    "    r = requests.get(url)\n",
    "    content = r.text\n",
    "    page_soup = BeautifulSoup(content, \"html.parser\")\n",
    "    articles_class = page_soup.find_all(class_=\"article-metaline\")\n",
    "    article_time = \"\"\n",
    "    find_time = False\n",
    "    for article in articles_class:\n",
    "        if article.find(\"span\", \"article-meta-tag\").string.find(\"時間\"):\n",
    "            find_time = True\n",
    "        elif find_time:\n",
    "            article_time = article.find(\"span\", \"article-meta-value\").string\n",
    "    article_time = str(article_time.encode('utf-8'))\n",
    "    year = article_time.split(\" \")[4][:-1]\n",
    "    return year\n",
    "\n",
    "all_articles_2017 = []\n",
    "all_articles_popular_2017 = []\n",
    "url = \"https://www.ptt.cc/bbs/Beauty/index2344.html\"  \n",
    "crawl(url, all_articles_2017, all_articles_popular_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231 [正妹] 彩美旬果 https://www.ptt.cc/bbs/Beauty/M.1514699566.A.E18.html\n",
      "1231 [正妹] 俄羅斯暗黑女模Helga Lovekaty https://www.ptt.cc/bbs/Beauty/M.1514702591.A.257.html\n",
      "1231 [正妹] 2018 https://www.ptt.cc/bbs/Beauty/M.1514729703.A.F6F.html\n",
      "1230 [正妹] IU https://www.ptt.cc/bbs/Beauty/M.1514607136.A.387.html\n",
      "1229 [正妹] 2017全球最美臉孔 下 https://www.ptt.cc/bbs/Beauty/M.1514493702.A.7DE.html\n",
      "1229 [正妹] 短髮護理師 https://www.ptt.cc/bbs/Beauty/M.1514515935.A.352.html\n"
     ]
    }
   ],
   "source": [
    "print (*all_articles_popular_2017,sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
